# Introduction

In previous lectures, we explored several concrete examples of stochastic processes. From the “simple random walk” of a particle on a one‑dimensional lattice in Lecture 1 to the “population dynamics” of birth–death processes in Lecture 4, we saw the central role of randomness in physical and biological systems. While these examples are specific, they raise a deeper question: Is there a universal mathematical framework, a common language, for evolution processes that “do not care about the past, only the present”?

The goal of this lecture is to establish and understand that common language. Erwin Frey will derive and explain two fundamental equations that govern all Markov processes. The first is the Chapman–Kolmogorov (CK) equation, which acts as a bridge in the macroscopic description, connecting probability distributions at different times. Then we move from the macroscopic to the microscopic and derive the Master Equation, which introduces instantaneous “rates” as a powerful tool to describe how state probabilities evolve continuously in time. Together, these two equations form the cornerstone of stochastic‑process theory.

# 1. Probabilistic Description of a Stochastic Process

## 1.1 Defining a Stochastic Process

To build a general theory, we start with a precise definition. A stochastic process can be viewed as a collection of random variables indexed by time t. In the most general case, it can be a multidimensional vector:

$$
X(t) = (X_1(t), X_2(t), \ldots)
$$

Each component $X_i(t)$ is itself a random variable. Its values may be real numbers ($X_i \in \mathbb{R}$), for example the position of a Brownian particle; or integers ($X_i \in \mathbb{Z}$), for example the number of individuals in a population.

To focus on core ideas, we follow the lecture and consider a single‑component random variable $X(t)$ for now.

## 1.2 Joint Probability Density: The Complete Description

How do we completely describe a stochastic process? The answer is: we need the joint probability that, at any sequence of times $t_1, t_2, \ldots, t_n$, the random variable $X(t)$ takes values $x_1, x_2, \ldots, x_n$. This is given by the joint probability density $P(x_n, t_n; x_{n-1}, t_{n-1}; \ldots; x_1, t_1)$ defined by

$$
P(x_n, t_n; \ldots; x_1, t_1)\, dx_n \cdots dx_1 = \mathrm{Prob}\{x_n \le X(t_n) \le x_n + dx_n; \ldots; x_1 \le X(t_1) \le x_1 + dx_1\}.
$$

This expression means the joint probability that the system is observed near $x_1$ at time $t_1$ and near $x_2$ at time $t_2$, …, and near $x_n$ at time $t_n$.

The joint probability density is the most fundamental and complete description of a stochastic process because it encodes all temporal correlations. But this “completeness” brings huge complexity. Determining this function for real systems requires measuring or computing the probabilities of all possible history paths. As the number of time points $n$ grows or the state space dimension increases, the complexity explodes, making it practically unusable. This exposes a core challenge in modeling: we must introduce physically motivated simplifications to extract a usable, solvable model from intractable complexity. The Markov property is precisely such a powerful simplification.

## 1.3 Stochastic Trajectory and Conditional Probability

To build intuition, consider a “stochastic trajectory”: each realization of the process $X(t)$ over time is one concrete path, i.e., one history of the system’s evolution. For example:

![Lecture blackboard screenshot](../assets/images/remote/wechat_2025-08-27_051544_045-1664f56938.png)

To analyze dependence across times—how the past influences the future—we use conditional probability. By definition, given a series of past events (states $x_1, \ldots, x_n$ at times $t_1, \ldots, t_n$), the probability of a series of future events (states $x_{n+1}, \ldots, x_{n+m}$ at times $t_{n+1}, \ldots, t_{n+m}$) is

$$
P(x_{n+m}, t_{n+m}; \ldots; x_{n+1}, t_{n+1} \mid x_n, t_n; \ldots; x_1, t_1) = \frac{P(x_{n+m}, t_{n+m}; \ldots; x_{n+1}, t_{n+1}; x_n, t_n; \ldots; x_1, t_1)}{P(x_n, t_n; \ldots; x_1, t_1)}.
$$

This will be our starting point for the Markov property.

# 2. Core Assumption: The Markov Property

## 2.1 Defining a “Memoryless” Process

The Markov property restricts the “memory” of a stochastic process and underpins everything that follows. Its core idea is memorylessness: the future depends only on the present, not on the entire past history that led to the present.

In terms of conditional probabilities,

$$
P(x_n, t_n \mid x_{n-1}, t_{n-1}; \ldots; x_1, t_1) = P(x_n, t_n \mid x_{n-1}, t_{n-1}).
$$

This implies that, to predict the probability of the system being at $x_n$ at time $t_n$, it suffices to know only its state $x_{n-1}$ at the immediately preceding time $t_{n-1}$. All earlier information ($x_{n-2}, \ldots, x_1$) is redundant for predicting the future. The present contains all information needed to predict the future.

This simple mathematical assumption reflects a deep physical insight. A system can be approximated as Markovian when there is a separation of time scales. For a large particle in a fluid (Brownian motion), the position $X(t)$ is our variable of interest. Strictly, the dynamics has memory because the velocity is correlated over short times; a complete description would track $(X(t), V(t))$. But collisions with many fluid molecules make the velocity correlation (memory) time very short. If we observe $X(t)$ on a time scale much longer than this, the next displacement depends almost only on the current position, not on detailed velocity history. Thus the Markov property is not an absolute law but an extremely useful approximation whenever unobserved, memory‑carrying “fast variables” relax quickly and do not affect the evolution of the “slow variables” we care about.

## 2.2 Simplifying the Joint Probability

The power of the Markov property is that it simplifies the unwieldy joint probability. By the chain rule of conditional probabilities,

$$
P(x_n, t_n; \ldots; x_1, t_1) = P(x_n, t_n \mid x_{n-1}, t_{n-1}; \ldots; x_1, t_1)\, P(x_{n-1}, t_{n-1}; \ldots; x_1, t_1).
$$

Applying the Markov property collapses the condition. Repeating this yields a product of two‑point transition probabilities:

$$
P(x_n, t_n; \ldots; x_1, t_1) = P(x_n, t_n \mid x_{n-1}, t_{n-1})\, P(x_{n-1}, t_{n-1} \mid x_{n-2}, t_{n-2}) \cdots P(x_2, t_2 \mid x_1, t_1)\, P(x_1, t_1).
$$

This is profound: the complete statistics of a Markov process are determined by two ingredients:

1. Initial condition: the probability distribution $P(x_1, t_1)$ at the starting time.
2. Transition rule: the conditional probabilities $P(x_j, t_j \mid x_{j-1}, t_{j-1})$ that map one state to the next.

We no longer need the intractable joint distribution over full histories. The evolution is a forward “propagation” of the initial distribution via transition probabilities.

# 3. Chapman–Kolmogorov Equation: A Bridge Across Times

Armed with the Markov property, we now derive the first fundamental equation: the Chapman–Kolmogorov (CK) equation.

## 3.1 Derivation: The Power of Marginalization

The derivation uses the law of total probability (marginalization). Consider three times $t_1 < t_2 < t_3$. We want $P(x_3, t_3 \mid x_1, t_1)$.

To go from $t_1$ to $t_3$, the system must pass some intermediate state $x_2$ at $t_2$. Since $x_2$ can be any state, we sum/integrate over all possibilities:

$$
P(x_3, t_3 \mid x_1, t_1) = \int dx_2\, P(x_3, t_3; x_2, t_2 \mid x_1, t_1).
$$

Using conditional probabilities and the Markov property,

$$
P(x_3, t_3; x_2, t_2 \mid x_1, t_1) = \frac{P(x_3, t_3; x_2, t_2; x_1, t_1)}{P(x_1, t_1)}.
$$

Apply the chain rule and the Markov property to the numerator:

$$
P(x_3, t_3; x_2, t_2; x_1, t_1) = P(x_3, t_3 \mid x_2, t_2)\, P(x_2, t_2 \mid x_1, t_1)\, P(x_1, t_1).
$$

Substitute back:

$$
P(x_3, t_3; x_2, t_2 \mid x_1, t_1) = P(x_3, t_3 \mid x_2, t_2)\, P(x_2, t_2 \mid x_1, t_1).
$$

Plug this into the marginalization to obtain the CK equation.

![Lecture blackboard screenshot](../assets/images/remote/a98a125d5c184ca583a184b82af04046-9e5f703177.png)

## 3.2 Form and Physical Meaning

Depending on whether the state is continuous or discrete, CK has two forms:

- Continuous state:

$$
P(x_3, t_3 \mid x_1, t_1) = \int dx_2\, P(x_3, t_3 \mid x_2, t_2)\, P(x_2, t_2 \mid x_1, t_1).
$$

- Discrete state (use $n$ for state and set $t_1=t_0,\, t_2=t',\, t_3=t$):

$$
P(n, t \mid n_0, t_0) = \sum_{n'} P(n, t \mid n', t')\, P(n', t' \mid n_0, t_0).
$$

Physical meaning: CK says that the evolution from $n_0$ to $n$ can be decomposed into two Markovian steps: first $n_0 \to n'$, then $n' \to n$. The core idea is “sum over all possible intermediate paths.” A long‑interval transition probability is the sum (or integral) of probabilities over all intermediate states, reflecting the composition rule of Markov evolution.

For discrete states, CK reveals a linear‑algebra structure. Define a transition matrix $\mathbf{T}(t_a\!\to\!t_b)$ with elements $(\mathbf{T})_{ij} = P(j, t_b \mid i, t_a)$. Then CK becomes matrix multiplication:

$$
\mathbf{T}(t_1 \to t_3) = \mathbf{T}(t_2 \to t_3)\, \mathbf{T}(t_1 \to t_2).
$$

This viewpoint is powerful: long‑time behavior (steady states) links directly to eigenvectors and eigenvalues of transition matrices.

# 4. From Macroscopic to Microscopic: The Master Equation

The CK equation describes probability evolution over a finite time interval. In many physical and chemical problems, we care about the instantaneous rate of change. Taking an infinitesimal time step in CK leads to the Master Equation.

## 4.1 From Finite Steps to Instantaneous Rates

Start from discrete CK and set $t'=t$ and $t=t+\Delta t$ with $\Delta t\to 0$:

$$
P(n, t+\Delta t \mid n_0, t_0) = \sum_m P(n, t+\Delta t \mid m, t)\, P(m, t \mid n_0, t_0).
$$

Consider the time derivative

$$
\frac{d}{dt} P(n, t \mid n_0, t_0) = \lim_{\Delta t\to 0} \frac{1}{\Delta t}\big[P(n, t+\Delta t \mid n_0, t_0) - P(n, t \mid n_0, t_0)\big].
$$

Insert CK and use $P(n, t \mid n_0, t_0) = \sum_m \delta_{n,m}\, P(m, t \mid n_0, t_0)$ to obtain

$$
\frac{d}{dt} P(n, t \mid n_0, t_0) = \lim_{\Delta t\to 0} \frac{1}{\Delta t} \sum_m \big[ P(n, t+\Delta t \mid m, t) - \delta_{n,m} \big] P(m, t \mid n_0, t_0).
$$

The bracket measures the short‑time change in probability to go from $m$ to $n$ in $\Delta t$.

## 4.2 The Transition Rate Matrix, Q

Define the transition‑rate matrix $\mathbf{Q}$ by

$$
Q(n,m) := \lim_{\Delta t\to 0} \frac{1}{\Delta t}\big[ P(n, t+\Delta t \mid m, t) - \delta_{n,m} \big].
$$

Interpretation:

- Off‑diagonals (Gain): for $n\ne m$, write $w_{m\to n}$,

$$
w_{m\to n} := Q(n,m) = \lim_{\Delta t\to 0} \frac{1}{\Delta t}\, P(n, t+\Delta t \mid m, t).
$$

$w_{m\to n}$ is the instantaneous transition rate from $m$ to $n$ (units $[\mathrm{time}]^{-1}$). The dimensionless probability to jump $m\to n$ in an infinitesimal $d\tau$ is $w_{m\to n}\, d\tau$; it contributes gain into $n$.

- Diagonals (Loss): for $n=m$,

$$
Q(m,m) = \lim_{\Delta t\to 0} \frac{1}{\Delta t}\big[ P(m, t+\Delta t \mid m, t) - 1 \big].
$$

Because “no jump” has probability near 1 at short times, $Q(m,m)<0$. Define the positive exit rate

$$
w_m := -Q(m,m) = \lim_{\Delta t\to 0} \frac{1}{\Delta t}\big[ 1 - P(m, t+\Delta t \mid m, t) \big].
$$

$w_m$ is the total rate to leave $m$ to anywhere else; it causes loss from $m$.

## 4.3 Final Form of the Master Equation

With $\mathbf{Q}$,

$$
\frac{d}{dt} P(n, t \mid n_0, t_0) = \sum_m Q(n,m)\, P(m, t \mid n_0, t_0).
$$

To expose gain–loss form, use probability conservation.

## 4.4 Manifestation of Probability Conservation

For any starting state $m$, total probability is conserved: $\sum_n P(n, t+\Delta t \mid m, t)=1$. Hence each column of $\mathbf{Q}$ sums to zero:

$$
\sum_n Q(n,m) = \lim_{\Delta t\to 0} \frac{1}{\Delta t}\Big[ \sum_n P(n, t+\Delta t \mid m, t) - \sum_n \delta_{n,m} \Big] = 0.
$$

Physically, this is probability conservation. The diagonal loss equals the sum of all off‑diagonal gains out of that state:

$$
-Q(m,m) = \sum_{n\ne m} Q(n,m) \quad \Rightarrow \quad w_m = \sum_{n\ne m} w_{m\to n}.
$$

Let $P_n(t) \equiv P(n, t \mid n_0, t_0)$. Using $w_{m\to n}=Q_{nm}$ and $w_n=-Q_{nn}=\sum_{m\ne n} w_{n\to m}$ gives

$$
\frac{d}{dt} P_n(t) = \underbrace{\sum_{m\ne n} w_{m\to n}\, P_m(t)}_{\text{Gain: from all } m \text{ into } n} \;-
\underbrace{\sum_{m\ne n} w_{n\to m}\, P_n(t)}_{\text{Loss: from } n \text{ to all } m}.
$$

This states: the rate of change of $P_n$ equals total inflow from all other states minus total outflow from $n$ to others.

## 4.5 Forward vs. Backward Master Equations

The equation derived above differentiates with respect to the “end” time $t$ ($\partial_t$), so it is called the forward Master Equation: given an initial condition, how does the future distribution evolve? There is also a backward Master Equation, with derivative with respect to the “start” time $t_0$ ($\partial_{t_0}$), answering a different question: given a fixed final state, how should the initial distribution be chosen? In physics and chemistry, the forward equation is more common because it naturally describes forward time evolution.

# 5. Python Simulation: Visualizing a Discrete Markov Process

To connect the “rates” in the Master Equation to actual random trajectories, we can simulate. For discrete‑state, continuous‑time Markov processes, the classic algorithm is the Gillespie algorithm (introduced in Lecture 3).

## 5.1 Model Setup

Consider a simple three‑state model (e.g., protein conformations): Open (O), Closed (C), and Inhibited (I). We label them 0, 1, 2. Transition rates:

$O \leftrightarrow C$: interconversion with rates $w_{0\to 1}=k_{OC}$ and $w_{1\to 0}=k_{CO}$.

$C \to I$: irreversible transition with rate $w_{1\to 2}=k_{CI}$.

The rate matrix $\mathbf{Q}$ is

$$
\mathbf{Q} = \begin{pmatrix}
-k_{OC} & k_{CO} & 0 \\
k_{OC} & -k_{CO} - k_{CI} & 0 \\
0 & k_{CI} & 0
\end{pmatrix}
$$

Each column sums to zero, ensuring probability conservation.

## 5.2 Algorithm and Code

The Gillespie idea: at any time, both “what happens next” and “how long to wait” are random. Two random draws determine the next event:

1. Waiting time: the total exit rate $W$ is the sum of all outgoing rates from the current state. The waiting time $\Delta t$ is exponential: $\Delta t = -\frac{1}{W}\ln r_1$, with $r_1\in(0,1]$.
2. Which event: a second uniform $r_2$ selects the transition proportional to its rate.

Python implementation:

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import FancyBboxPatch, ArrowStyle
import matplotlib.patches as mpatches
from collections import Counter

def simulate_markov_process(initial_state, rates, t_max):
    """
    Simulate a discrete state, continuous time Markov process using the Gillespie algorithm.

    Parameters:
        initial_state (int): Initial state of the system (0, 1 or 2).
        rates (dict): Dictionary of transition rates, e.g. {'k_oc': 1.0,...}.
        t_max (float): Maximum simulation time.

    Returns:
        tuple: A tuple containing lists of times and states.
    """
    # Unpack rate parameters
    k_oc = rates.get('k_oc', 0)
    k_co = rates.get('k_co', 0)
    k_ci = rates.get('k_ci', 0)

    # Store trajectory data
    times = [0.0]
    states = [initial_state]

    current_time = 0.0
    current_state = initial_state

    while current_time < t_max:
        # Define possible transitions and their rates in the current state
        possible_transitions = []
        if current_state == 0:  # State O
            possible_transitions = [(1, k_oc)]
        elif current_state == 1:  # State C
            possible_transitions = [(0, k_co), (2, k_ci)]
        elif current_state == 2:  # State I (absorbing state)
            # No outward transitions from state I
            break

        # Calculate total exit rate W
        total_rate = sum(rate for _, rate in possible_transitions)

        if total_rate == 0:
            # Absorbing state, simulation ends
            break

        # Step 1: Determine time interval to next event
        r1 = np.random.rand()
        dt = -np.log(r1) / total_rate
        
        current_time += dt
        if current_time > t_max:
            break

        # Step 2: Determine which event occurs
        r2 = np.random.rand()
        cumulative_prob = 0.0
        next_state = -1
        for state, rate in possible_transitions:
            cumulative_prob += rate / total_rate
            if r2 < cumulative_prob:
                next_state = state
                break

        # Update state and store results
        current_state = next_state
        times.append(current_time)
        states.append(current_state)

    return times, states

def plot_state_diagram(rates):
    """
    Plot state transition diagram
    """
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    # State positions
    positions = {0: (2, 4), 1: (2, 2), 2: (2, 0)}  # O, C, I
    state_names = {0: 'Open (O)', 1: 'Closed (C)', 2: 'Inhibited (I)'}
    colors = {0: '#FF6B6B', 1: '#4ECDC4', 2: '#45B7D1'}
    
    # Draw state nodes (larger circles)
    for state, pos in positions.items():
        circle = plt.Circle(pos, 0.5, color=colors[state], alpha=0.8)  # Increased radius from 0.3 to 0.5
        ax.add_patch(circle)
        ax.text(pos[0], pos[1], state_names[state], ha='center', va='center', 
                fontsize=10, fontweight='bold', color='white')
    
    # Draw transition arrows
    k_oc = rates.get('k_oc', 0)
    k_co = rates.get('k_co', 0)
    k_ci = rates.get('k_ci', 0)
    
    # O <-> C
    if k_oc > 0:
        arrow1 = mpatches.FancyArrowPatch((2, 3.5), (2, 2.5), 
                                          arrowstyle='->', mutation_scale=20, 
                                          color='gray', alpha=0.7, 
                                          connectionstyle="arc3,rad=-0.3")
        ax.add_patch(arrow1)
        ax.text(2.5, 3, f'k_OC = {k_oc}', fontsize=9, rotation=-90, ha='center')
    
    if k_co > 0:
        arrow2 = mpatches.FancyArrowPatch((2, 2.5), (2, 3.5), 
                                          arrowstyle='->', mutation_scale=20, 
                                          color='gray', alpha=0.7,
                                          connectionstyle="arc3,rad=-0.3")
        ax.add_patch(arrow2)
        ax.text(1.5, 3, f'k_CO = {k_co}', fontsize=9, rotation=90, ha='center')
    
    # C -> I
    if k_ci > 0:
        arrow3 = mpatches.FancyArrowPatch((2, 1.5), (2, 0.5), 
                                          arrowstyle='->', mutation_scale=20, 
                                          color='gray', alpha=0.7)
        ax.add_patch(arrow3)
        ax.text(2.5, 1, f'k_CI = {k_ci}', fontsize=9, rotation=-90, ha='center')
    
    ax.set_xlim(0, 4)
    ax.set_ylim(-0.5, 5)
    ax.set_aspect('equal')
    ax.axis('off')
    ax.set_title('State Transition Diagram', fontsize=14, fontweight='bold')
    
    return fig

def plot_multiple_trajectories(initial_state, rates, t_max, num_trajectories=5):
    """
    Plot multiple trajectories to show randomness
    """
    fig, ax = plt.subplots(1, 1, figsize=(12, 6))
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
    state_labels = ['Open (O)', 'Closed (C)', 'Inhibited (I)']
    
    for i in range(num_trajectories):
        times, states = simulate_markov_process(initial_state, rates, t_max)
        # Add small offset to each trajectory for better visualization
        offset_states = [s + (i*0.05) for s in states]
        ax.step(times, offset_states, where='post', alpha=0.7, 
                color=colors[i % len(colors)], linewidth=1.5)
    
    ax.set_yticks([0, 1, 2])
    ax.set_yticklabels(state_labels)
    ax.set_xlabel('Time')
    ax.set_ylabel('State')
    ax.set_title(f'Multiple Stochastic Trajectories (N={num_trajectories})')
    ax.grid(True, alpha=0.3)
    
    return fig

def plot_state_probability_distribution(initial_state, rates, t_max, num_samples=1000):
    """
    Plot state probability distribution
    """
    # Run multiple simulations to get statistics
    final_states = []
    all_times = []
    all_states = []
    
    for _ in range(num_samples):
        times, states = simulate_markov_process(initial_state, rates, t_max)
        if times:
            final_states.append(states[-1])
            all_times.extend(times)
            all_states.extend(states)
    
    # Count final state distribution
    state_counts = Counter(final_states)
    total = sum(state_counts.values())
    state_probs = {state: count/total for state, count in state_counts.items()}
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Final state distribution
    state_labels = ['Open (O)', 'Closed (C)', 'Inhibited (I)']
    state_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    probs = [state_probs.get(i, 0) for i in range(3)]
    
    bars = ax1.bar(state_labels, probs, color=state_colors, alpha=0.7)
    ax1.set_ylabel('Probability')
    ax1.set_title(f'Steady-State Distribution (t={t_max}, N={num_samples})')
    ax1.set_ylim(0, 1)
    
    # Add value labels on bars
    for bar, prob in zip(bars, probs):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{prob:.2f}', ha='center', va='bottom')
    
    # State occupancy over time (simplified)
    time_points = np.linspace(0, t_max, 50)
    state_occupancy = {0: [], 1: [], 2: []}
    
    for t in time_points:
        count = {0: 0, 1: 0, 2: 0}
        total_count = 0
        for _ in range(100):  # Sample 100 trajectories at each time point
            times, states = simulate_markov_process(initial_state, rates, t_max)
            # Find state at time t
            state_at_t = states[0]  # Default to initial state
            for i in range(len(times)-1):
                if times[i] <= t < times[i+1]:
                    state_at_t = states[i]
                    break
                elif t >= times[-1]:
                    state_at_t = states[-1]
                    break
            count[state_at_t] += 1
            total_count += 1
        
        for state in [0, 1, 2]:
            state_occupancy[state].append(count[state] / total_count if total_count > 0 else 0)
    
    # Plot state occupancy over time
    for state in [0, 1, 2]:
        ax2.plot(time_points, state_occupancy[state], 
                label=state_labels[state], color=state_colors[state], linewidth=2)
    
    ax2.set_xlabel('Time')
    ax2.set_ylabel('Probability')
    ax2.set_title('State Probability Evolution')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    return fig

# --- Simulation parameters ---
params = {
    'k_oc': 0.5,  # Rate O -> C
    'k_co': 0.2,  # Rate C -> O
    'k_ci': 0.1   # Rate C -> I
}
initial_state = 0  # Start from state O
simulation_time = 100.0

# --- Create enhanced visualizations ---
# 1. State transition diagram
fig1 = plot_state_diagram(params)
plt.show()

# 2. Multiple trajectories
fig2 = plot_multiple_trajectories(initial_state, params, simulation_time, num_trajectories=10)
plt.show()

# 3. State probability distribution
fig3 = plot_state_probability_distribution(initial_state, params, simulation_time, num_samples=500)
plt.show()
```

Dynamics of the protein system: the transition from Closed to Inhibited is irreversible, while Open and Closed interconvert.
![Code execution output](../assets/images/remote/Figure_29-8827d54923.png)

**Stochasticity:** Different trajectories occupy different states at the same time, reflecting randomness.

**State jumps:** The system switches instantaneously between states, consistent with Markov dynamics.

**Absorbing state:** All trajectories eventually end in the Inhibited state because it is absorbing.

**Dwell time:** The residence time in each state is random and exponentially distributed.

![Code execution output](../assets/images/remote/Figure_3-07d095fd5e.png)

This shows the final‑state distribution after long evolution ($t=100$). All samples end in the Inhibited state with probability near 1.0; probabilities of Open and Closed are near 0, indicating eventual capture by the absorbing state.

# Summary

![Lecture blackboard screenshot](../assets/images/remote/6a550de509f14df988a27e035070ca51-f0e32b12c1.png)

This lecture built a logical path from a universal description to concrete dynamical equations:

1. Start from the most complete yet most complex description: the joint probability density.
2. Introduce the key physical simplification—the Markov property (memorylessness).
3. Based on the Markov property, derive the first fundamental equation—the Chapman–Kolmogorov equation. It is a sum/integral equation for finite time steps, with the core idea “sum over all intermediate paths.”
4. Take the infinitesimal‑time limit of CK to obtain the Master Equation. It is a differential equation in terms of transition rates, expressing instantaneous change as a gain–loss balance among states.

To contrast the two equations:

| Feature | Chapman–Kolmogorov equation | Master Equation |
| --- | --- | --- |
| Time scale | Finite step ($t \to t+\Delta t$) | Infinitesimal step ($dt$) |
| Object | Transition probability $P(n, t+\Delta t \mid m, t)$ | Transition rate $w_{m\to n}$ |
| Math form | Sum/integral equation | Differential equation |
| Core idea | Sum over intermediate paths | Balance of probability flux |

The Master Equation is not the end of theory but the start of applications. With it we can:

- Solve for steady‑state distributions: probabilities as $t\to\infty$ (often the zero‑mode of the equation).
- Compute relaxation times: how long after a perturbation to return to steady state (linked to eigenvalues of the rate matrix).
- Analyze first‑passage times: average time to reach a target state for the first time.

These questions are central across physics, chemistry, biology, and finance.

